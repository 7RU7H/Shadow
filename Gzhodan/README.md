# README

Gzhodan is the G*-olang-*zombified Hyper-Optimized Daily Aggregating Newsreporter, because names are difficult, golang is good and Pseudo-Humaness is the objective. 

The hope is that this will become middleware for data-crunching, database and LLMs. For now and probably always this is just for CVE, HAcking, InfoSec, CyberSecurity information gathering from trusted URLs because all other news is just noise to a new zero day or vulnerability. 



## Design 

- Find how scarecrow got there picture, needs a good repo picture. 
- The name is bad. The name needs to be on the Daily Swig level of naming 
- I want the data to be easy to just hand to another Go lang backend to an AI to do all AI related Software advances the Messlier is hyping me up for not have read emails and news for. A glorious future I will live to enjoy.

- package with a gzlop.go so no additional downloads and ensure development of gzlop
- package with gurl so that is developed.

- Python3 -> GO GO GO - gzlop for the win! for recursive grep-like in go routines and memory arena
- Just avoid the X/Twitter issues of some of infosec being X/Twitter and x/Twitter is still X/Twitter.



####  Dealing with the web application outliner
    - ycombinator Hacker News - non hackery tidbits by a set number of points 300+ because I only want the suggestion to go to another site, I am not going to comment, but other might and if they want lower the threshold for using this site they with a flag ASP or fork.
        - https://news.ycombinator.com/?p=1
        - https://news.ycombinator.com/?p=2
        - https://news.ycombinator.com/?p=3    
        - by X points

    - SAN @Risk
        - https://www.sans.org/newsletters/at-risk/
        - Roman numerals url /at-risk/
            - YEAR-ISSUE
            - 2024 volume 2 would be xxiv-02   
            - Sometimes there are 49-50?
            - Do some weeks in the year check on 48 - 52

- Daily Swig 
    - https://portswigger.net/daily-swig/vulnerabilities  - this web security news
    - https://portswigger.net/daily-swig/cloud-security
    - https://portswigger.net/daily-swig/supply-chain-attacks
    - https://portswigger.net/daily-swig/network-security
    - https://portswigger.net/daily-swig/zero-day

https://thehackernews.com/search?max-results=20

https://arstechnica.com/security/

## Objectives

- go std as much as possible - glzop being the except
- Operate as much like a normal user (from the point of a remote site) - develop the Gzombie User library
- Bypass all future the rate limiting by smart path traversal of complete url list and being slow using a custom cli-browser & curl blend that has jitter and user-agent 
    - MAC and IP address randomisation  
  
titles links

- Ingestion
- Aggregate from modifiable configuration
    - For each site by url and date from previous log by yesterdays date - if no previous then grab, prevent previous urls continuously being featured. 
    - Token , case, US and UK, 
    - Tokens in the memory arenas fo 1.2X for the glzoping 
        - CVE, Exploit, Zero-Day, 0day,etc, Attack, Vuln ,Vulnerablilities, Malware, Ransomware, PoC, RCE, Bug , exploit,cve,rce,sqli,xss,lfi,ssti,ssrf,csrf,xxe,traversal,crlf,csv,injection,deserialization, JSON web Token, cookie (paramatre, parameter, prototype) pollution,smuggling, leaks, breach, errors, misconfiguration, API Key, crlf, cvs, clickjacking, DNS Rebinding, supply chain, Dom clubbering, file inclusion, insecure, Race condition, Tabnabbing, Type Juggling, (upload file - yikes), XSLT, Open Redirect, Mass Assignment, Crytography, Account takeover, Zero Click, Privilge Escalation, Decrypt, Encryption
        
        - Store all urls and summarised statements, Tokens hit! Date, - for self-reference in ingestion and for general collection
- Printing Terminal colourful friendly Newletter and Markdown friendly 

## Timeline of TODO

Pieces then Asynchoronous HARMONY!

urls.txt

0. Develop the Gzombie
    - How to develop the HUMANESS of firefox/curl cli browsing the internet from the CLI
        - Browser extensions 
            - No JS and configure   
        - Jitter that match    
    - How to test the HUMANESS of firefox cli browsing the internet from the CLI         
    - Queueing of tasks
        - What is the safest traversal over all urls to avoid rate limiting (a(1-20),b(1,1,2,3,4,5)(1-5)...e(1)) - e(1) being SANS risk b being portswigger
        - just a array sequencer rather than a graph
1. Develop the ingestion
    - curl to buffer
    - MODIFY https://cs.opensource.google/go/go/+/refs/tags/go1.21.6:src/net/http/client.go;l=483 
2. Develop the logging and data retention
    - application.log
    - storage.log
    - markdown newletter generation
    - Diretory stucture for:
        - YEAR-MONTH-DAY.txt    
3. Aggregation
    - glzop Library
    - Tokens as a const 
    - Memory arena hash map regex 
    - Check *correct article* storage.log file with glzop library
4. Article parsing 
    - https://github.com/anaskhan96/soup - DO NOT USE TO GET ANYTHING - just copy functionality to parse 
5. Intelligence coalition 
    - `firefox -screenshot $URL --headless --window-size=1920,1080`
        - Also very useful for handing to a AI to look at 

## Inspirations, Ideas, etc


https://www.educative.io/answers/how-to-implement-a-queue-in-golang
https://dev.to/vearutop/memory-arenas-in-go-j1f

https://github.com/anaskhan96/soup



- Use bug bounty screenshotter to get artitical screenshots for markdown report or just 
- GL with 3 year old web driver for golang that proabbly wont work https://github.com/tebeka/selenium 