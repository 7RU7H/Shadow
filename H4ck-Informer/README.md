# README

## Design 

- Find how scarecrow got there picture, needs a good repo picture. 
- The name is bad. The name needs to be on the Daily Swig level of naming 
- I want the data to be easy to just hand to another Go lang backend to an AI to do all AI related Software advances the Messlier is hyping me up for not have read emails and news for. A glorious future I will live to enjoy.

- package with a gzlop.go so no additional downloads and ensure development of gzlop

- Python3 -> GO GO GO - gzlop for the win! for recursive grep-like in go routines and memory arena

- How avoid the X/Twitter issues of some of infosec being X/Twitter and x/Twitter is still X/Twitter.

- Dealing with the web application outliner
    - ycombinator Hacker News - non hackery tidbits by a set number of points 300+ because I only want the suggestion to go to another site, I am not going to comment, but other might and if they want lower the threshold for using this site they with a flag ASP or fork.
        - https://news.ycombinator.com/?p=1
        - https://news.ycombinator.com/?p=2
        - https://news.ycombinator.com/?p=3    
        - by X points

    - SAN @Risk
        - https://www.sans.org/newsletters/at-risk/
        - Roman numerals url /at-risk/
            - YEAR-ISSUE
            - 2024 volume 2 would be xxiv-02   
            - Sometimes there are 49-50?
            - Do some weeks in the year check on 48 - 52

- Daily Swig 
    - https://portswigger.net/daily-swig/vulnerabilities  - this web security news
    - https://portswigger.net/daily-swig/cloud-security
    - https://portswigger.net/daily-swig/supply-chain-attacks
    - https://portswigger.net/daily-swig/network-security
    - https://portswigger.net/daily-swig/zero-day

https://thehackernews.com/search?max-results=20

https://arstechnica.com/security/

## Objectives

- go std as much as possible - glzop being the except
- Operate as much like a normal user (from the point of a remote site) - develop the Gzombie User library
- Bypass all future the rate limiting by smart path traversal of complete url list and being slow using a custom cli-browser & curl blend that has jitter and user-agent 
    - MAC and IP address randomisation  
  
titles links

- Ingestion
- Aggregate from modifiable configuration
    - For each site by url and date from previous log by yesterdays date - if no previous then grab, prevent previous urls continuously being featured. 
    - Token , case, US and UK, 
    - Tokens in the memory arenas fo 1.2X for the glzoping 
        - CVE, Exploit, Zero-Day, 0day,etc, Attack, Vuln ,Vulnerablilities, Malware, Ransomware, PoC, RCE, Bug , exploit,cve,rce,sqli,xss,lfi,ssti,ssrf,csrf,xxe,traversal,crlf,csv,injection,deserialization, JSON web Token, cookie (paramatre, parameter, prototype) pollution,smuggling, leaks, breach, errors, misconfiguration, API Key, crlf, cvs, clickjacking, DNS Rebinding, supply chain, Dom clubbering, file inclusion, insecure, Race condition, Tabnabbing, Type Juggling, (upload file - yikes), XSLT, Open Redirect, Mass Assignment, Crytography, Account takeover, Zero Click, Privilge Escalation, Decrypt, Encryption
        
        - Store all urls and summarised statements, Tokens hit! Date, - for self-reference in ingestion and for general collection


- Printing Terminal colourful friendly Newletter and Markdown friendly 